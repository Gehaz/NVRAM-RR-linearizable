
\section{Introduction}

\emph{Shared-memory multiprocessors} are now prevalent anywhere from high-end server machines, through desktop and laptop computers to smartphones, accelerating the shift to concurrent, multi-threaded software.
Concurrent software involves a collection of threads, each running a separate piece of code, possibly on a different core. Since threads may be delayed because of a variety of reasons (such as cache-misses, interrupts, page-faults, and scheduler preemption),
shared-memory multiprocessors are \emph{asynchronous} in nature.

Asynchrony is also related to reliability, since asynchronous algorithms that provide nonblocking progress properties (e.g., lock-freedom and wait-freedom) in an asynchronous environment with reliable processes continue to provide the same progress properties when \emph{crash failures} are introduced.
Informally speaking, this property holds because a process that crashes permanently at an arbitrary point in the execution of its algorithm is indistinguishable to the other processes from one that is merely very slow. Owing to its simplicity and intimate relationship with asynchrony, the crash-failure model is almost ubiquitous in the treatment of non-blocking shared memory algorithms.

The attention to the crash-failure model has so far mostly neglected the \emph{crash-recovery} model, in which a failed process may be resurrected after it crashes. The reason is that the crash-recovery model is poorly matched to multi-core
architectures with volatile SRAM-based caches and DRAM-based main memories: Any state stored in main memory is lost entirely in the event of a system crash or power loss, and recording recovery information in non-volatile secondary storage (e.g., on a hard disk drive or solid state drive) imposes overheads that are unacceptable for performance-critical tasks, such as synchronizing threads inside the operating system kernel.

This separation between volatile main memory and non-volatile secondary storage has led to a partitioning of the program state into operational data stored using in-memory data structures, and recovery data stored using sequential on-disk structures such as transaction logs.
In the event of a system-wide failure, such as a power outage, in-memory data structures are lost and must be reconstructed entirely from the recovery data, making the software system that relies on them temporarily unavailable. As a result, the design of in-memory data structures emphasizes disposable constructs optimized for parallelism, in contrast to the structures that hold recovery data, which cannot be discarded upon a failure and which benefit less from parallelism since their performance is limited by the secondary storage.

Recent developments in non-volatile main memory (NVRAM) media foreshadow the eventual convergence of primary and secondary storage into a single layer in the memory hierarchy, combining the performance benefits of conventional main memory with the durability of secondary storage. Traditional log-based recovery techniques can be applied correctly in such systems but fail to take full advantage of the parallelism enabled by allowing processing cores to access recovery data directly using memory operations rather than slow block transfers from secondary storage.

In this paper we investigate how the performance benefits of NVRAM can be harnessed for improving the robustness of shared-memory programs by allowing concurrent algorithms to efficiently recover from crash failures. This challenge requires a careful rethinking of recovery mechanisms and involves addressing both foundational and algorithmic research questions. Our emphasis would be on direct, non-transactional, approaches of implementing algorithms for \emph{recoverable concurrent} (also called \emph{persistent}) objects.
More specifically, we formulate an abstract model of NVRAM shared-memory multiprocessors, and discuss known correctness and progress conditions, while proposing a new property called \textit{recoverable-response linearizability}. We then discuss the theoretic limitations of these systems, on the one hand, and construction of concurrent algorithms that support effective recovery from crash failures, on the other hand. We focus on construction of recoverable versions of primitive memory operations such as read, write, swap, compare-and-swap and fetch-and-add, deriving upper and lower bounds, and how these recoverable versions can be used to implement various applications.

\remove{
In the rest of this section, we provide required background.
First, we briefly describe the standard shared-memory model.
This is followed by a survey of correctness conditions that
were proposed in recent years for defining the behavior
of recoverable concurrent objects.
We then survey previous work on transactional access to recoverable
concurrent objects as well as more direct, non-transactional,
implementations of such objects.}



\subsection{Related work}

When an application implements a concurrent data structure, it is necessary to specify its \emph{semantics}, namely, the properties provided by values it returns, which determine the feasibility and complexity of implementing it. The \emph{correctness} condition of a concurrent data structure specifies how to derive the semantics of concurrent implementations of the data structure from the corresponding \emph{sequential specification} of the data structure. This requires to disambiguate the expected results of concurrent operations on the data structure.

Two common ways to do so are \emph{sequential consistency}~\cite{LamportSC} and \emph{linearizability}~\cite{herlihyWingLinearizability}. Both require that the values returned by the operations appear to have been returned by a sequential execution of the same operations; sequential consistency only requires this order to be consistent with the order in which each individual thread invokes the operations, while linearizability further requires this order to be consistent with the real-time order of non-overlapping operations. The standard shared-memory model assumes that shared memory is linearizable or at least sequentially consistent.

Linearizability is ill-equipped to specify the correctness criteria for implementations that support crash-recovery failures, since linearizability has no notion of an aborted or failed operation, and requires that a process finish one operation before it invokes the next. Extended versions of linearizability or, more generally, alternative definitions are required for specifying correctness for such implementations.

Aguilera and Fr{\o}lund~\cite{Aguilera2003StrictLA} proposed \emph{strict linearizability} as a correctness condition for persistent concurrent objects,
which treats the crash of a process as a response, either successful or unsuccessful, to the interrupted operation. A successful response means that the operation takes effect at some point between its invocation and the crash failure, and an unsuccessful response means that the operation does not take effect at all.
Strict linearizability preserves both \emph{locality}~\cite{herlihy91waitfree}
and program order. Although this property seems like a natural extension for linearizability, Aguilera and Fr{\o}lund proved that unlike linearizability it precludes the implementation of some wait-free objects for certain machine models:
there is no wait-free implementations of multi-reader single-writer (MRSW) registers from single-reader single-writer (SRSW) registers under strict linearizability.

Guerraoui and Levy~\cite{DBLP:conf/icdcs/GuerraouiL04} proposed \emph{persistent atomicity}. It is similar to strict linearizability, but allows an operation interrupted by a failure to take effect before the subsequent invocation of the same process, possibly after the failure. They also proposed \emph{transient atomicity}, which relaxes this criterion even further and allows an interrupted operation to take effect before the subsequent write response of the same process. Both conditions ensure that the state of an object will be consistent in the wake of a crash, but they fail to provide locality: correct histories of separate objects, when merged, will not necessarily yield a correct composite history.

Guerraoui and Levy properties capture, in some sense, the flexibility of implementations which uses helping mechanism, where an interrupted operation by some failed process $p$ can be completed by a different process after $p$ crushes. Hence, we would like to have the option to set the linearization point after the crush of $p$, where strict linearizability does not allow so. Moreover, Censor-Hillel et al. \cite{DBLP:conf/podc/Censor-HillelPT15} formalised the notion of helping, and proved that some objects, when implemented wait-free, must use an helping mechanism.

In addition, it is not clear how to generalize transient atomicity to a general object which does not support a write operation, as the definition explicitly uses write operation in order to determine the allowed interval for the linearization point. Moreover, transient atomicity allows a scenario where a process $p$ crush during an operation on object $X$, then recovers and complete a read operation on $X$, while the linearization point of the interrupted operation is set to after the read. That is, we get a history where $p$ executes two overlapping operations on the same object, causing a program order inversion, and violets the well-formedness property (defined in Section...), which is in the heart of the linearizability definition.

In order to overcome the above drawbacks Berryhill et al. \cite{DBLP:conf/opodis/BerryhillGT15} proposed an alternative condition, called \emph{recoverable linearizability}, which achieves locality but may sacrifice program order after a crash. It is a relaxed version of persistent atomicity, which requires the operation to be linearized or aborted before any subsequent linearization by the pending thread on that same object.

Izraelevitz et al.~\cite{DBLP:conf/wdag/IzraelevitzMS16} considered
a real-world failure model, in which processes are assumed to fail together,
as part of a full-system crash.
Under this model, persistent atomicity and recoverable linearizability
are indistinguishable (and thus local).
The term \emph{durable linearizability} was used to refer to this merged
consistency condition under the restricted failure model.

%Berryhill et al. proposed \emph{recoverable linearizability} (or \emph{R-linearizability}) %\cite{DBLP:conf/opodis/BerryhillGT15}, a correctness condition that allows an operation to be linearized after %the failure that interrupted it. R-linearizability satisfies locality and also maintains program order.




