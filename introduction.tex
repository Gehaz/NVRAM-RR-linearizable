\section{Introduction}

Shared-memory multiprocessors are asynchronous in nature.
Asynchrony is related to reliability,
since algorithms that provide nonblocking progress properties
(e.g., lock-freedom and wait-freedom \cite{herlihy91waitfree})
in an asynchronous environment with reliable processes
continue to provide the same progress properties in the presence
of \emph{crash failures}.
This happens because a process that crashes
permanently during the execution of the algorithm is indistinguishable
to the other processes from one that is merely very slow.
Owing to its simplicity and intimate relationship with asynchrony,
the crash-failure model is almost ubiquitous in the treatment of
concurrent algorithms.

The attention to the crash-failure model has so far mostly neglected
the \emph{crash-recovery} model, in which a failed process may be
resurrected after it crashes.
%The reason is that the crash-recovery model is poorly matched to
%contemporary multi-core architectures with volatile SRAM-based caches
%and DRAM-based main memories:
%Any state stored in main memory is lost entirely in the event of
%a system crash or power loss, and recording recovery information in
%non-volatile secondary storage (e.g., hard disk drive or solid state drive)
%imposes overheads that are unacceptable for performance-critical tasks,
%such as synchronizing threads inside the operating system kernel.
%
%This separation between volatile main memory and non-volatile secondary storage has led to a partitioning of the program state into operational data stored using in-memory data structures, and recovery data stored using sequential on-disk structures such as transaction logs.
%In the event of a system-wide failure, such as a power outage, in-memory data structures are lost and must be reconstructed entirely from the recovery data, making the software system that relies on them temporarily unavailable. As a result, the design of in-memory data structures emphasizes disposable constructs optimized for parallelism, in contrast to the structures that hold recovery data, which cannot be discarded upon a failure and which benefit less from parallelism since their performance is limited by the secondary storage.
Recent developments foreshadow the emergence of new systems,
in which byte-addressable \emph{non-volatile main memory} (\emph{NVRAM}),
combining the performance benefits of conventional main memory
with the durability of secondary storage,
co-exists with (or eventually even replaces) traditional volatile memory.
Consequently, there is increased interest in \emph{recoverable concurrent
objects} (also called \emph{persistent} or \emph{durable}):
objects that are made robust to crash-failures by allowing their operations
to recover from such failures.

In this paper, we present a novel abstract individual-process crash-recovery model for non-volatile memory, inspired by a model introduced by Golab and Ramaraju for studying the \emph{recoverable mutual exclusion} (RME) problem~\cite{GolabR16}. Our model enables the composition of recoverable objects, so that complex recoverable objects can be constructed in a modular manner from simpler recoverable base objects.
We assume that processes communicate via persistent shared-memory variables.
Each process also has local variables stored in volatile processor registers. At any point, a process may incur a crash-failure, causing all its local variables to be reset to arbitrary values.
A key challenge with which recovery code must cope in our model is
that operation response values are returned via volatile
processor registers. Hence, they may become inaccessible to the calling process if it fails just before persisting the response value. Each recoverable operation $Op$ is associated with a \emph{recovery function} that is responsible for completing $Op$ upon recovery from a crash-failure.
%Our model enables the composition of recoverable objects,
%so that complex recoverable objects can be constructed in a modular manner from simpler recoverable base %objects.


Several correctness conditions for the crash-recovery model were defined in recent years (see, e.g., \cite{Aguilera2003StrictLA,DBLP:conf/opodis/BerryhillGT15,DBLP:conf/icdcs/GuerraouiL04,DBLP:conf/wdag/IzraelevitzMS16}). The goal of these conditions is to maintain the state of concurrent objects consistent in the face of crash failures. However,
guaranteeing object consistency is insufficient for composability.
%none of them guarantees composability. Composability requires that the recovery code is always able to infer whether the failed operation completed or notand, if so, to obtain its response value. Inferring this may be challenging, since operation responses are returned via volatile processor registers.
To illustrate this point, consider a base object $\cal{B}$ that supports 
atomic \emph{compare\&swap} (CAS) and \emph{read} operations.
Suppose that an operation $Op$ by some recoverable object $\cal{O}$ invokes ${\cal{B}}.compare\&swap(old,new)$
in its algorithm which, upon completion,
should return a success or failure response.
Suppose also that some process $p$ crashes inside $Op$, immediately after it invokes
${\cal{B}}.compare\&swap(old,new)$.
Since the CAS operation is atomic, the consistency of ${\cal{B}}$ is ensured in
spite of $p$'s failure: either the CAS took effect before the failure, or it did not. 
%Consequently, the resulting execution complies with the above definitions.
However, the consistency of ${\cal{B}}$ is insufficient, since once $p$ recovers from the 
crash, it has no way of knowing whether the CAS succeeded or not, as its response was written to a volatile local variable that was lost because of the crash. Reading ${\cal{B}}$ will not help, since even if \emph{new} was written before the crash, it may have been overwritten by other processes since then. $Op$   may therefore not be able to proceed correctly.

To address this issue, we define within the framework of our model
the notion of \emph{composable recoverable linearizability} (CRL),
a novel correctness condition that captures the requirements
for recoverable objects composability.
Informally, CRL allows the recovery code to extend the interval
of the failed operation until the recovery code succeeds to complete
(possibly after multiple failures and recovery attempts). CRL implies that,
following recovery, an implemented (higher-level) recoverable operation
is able to complete its invocation of a base-object operation and obtain its response.
%It requires a transactional effect guaranteeing that
%either the operation is linearized at some point between its invocation
%and the completion of recovery code or it has no effect.

%A key challenge with which recovery code must cope in our model is
%the following: operation response values are returned via volatile
%processor registers and may therefore become inaccessible to the
%calling process if it fails just before persisting the response value.

%The CRL condition implies that, following recovery, a process is able to complete its
%operation and obtain its response.

We present several algorithms for \emph{composable recoverable primitive objects},
recoverable versions of widely-used primitive shared-memory operations such as
read, write, test-and-set and compare-and-swap,
that can be used by higher-level recoverable objects.
We also provide an example of how these recoverable base objects can be
used for constructing a recoverable counter object.

In addition, we prove an impossibility result on wait-free implementations of recoverable test-and-set (TAS) objects from read, write and TAS operations, thus demonstrating that our model also facilitates
rigorous analysis of the limitations of recoverable concurrent objects.

\remove{%%%%%%%
Similarly to how the standard shared-memory model abstracts away
the need for using barriers required for maintaining program order,
our model abstracts away the need for using persistence barrier instructions.
While low-level barrier instructions are required in real-world
multiprocessors, the more abstract model allows capturing
many of the theoretical limitations of real-world systems,
as well as devising concurrent algorithms that can be adapted relatively
easily to work on them.
} %%%%%%%%%% END REMOVE

