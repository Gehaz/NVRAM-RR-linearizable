
\section{Model and Definitions}
\label{section: Model}

\subsection{Standard Shared-Memory Model}

We use a standard model, based on Herlihy and Wing's model \cite{herlihyWingLinearizability}, of an asynchronous shared memory system. A set $P$ of $n > 1$ \textit{processes} $p_0, \ldots, p_{n-1}$ communicate by applying operations on shared \textit{base objects} that support atomic operations, e.g., reads, writes and read-modify-write, to shared variables. No bound is assumed on the size of a shared variable (i.e., the number of distinct values it can take).
Base objects are used in order to construct more complex implemented objects, such as queues and stacks, by defining access procedures that simulate each operation on the implemented object using operations on base objects.

The interaction of processes with implemented objects is modelled using steps and histories. There are four types of steps:
\begin{inparaenum}[(1)]
	\item an invocation step, denoted $(INV, p, X, op)$, represents the invocation by process $p$ of operation $op$ on implemented object $X$;
	\item a response step, denoted $(RES, p, X, ret)$, represents the completion by process $p$ of the last operation it invoked on object $X$, with response $ret$;
	\item a crash step, denoted $(CRASH, p)$, represents the crash of a processes $p$;
	\item a recovery step, denoted $(REC, p)$, represents the recovery of a process $p$. This step is the only one allowed by $p$ after a $(CRASH, p)$ step.
\end{inparaenum}

A \textit{history} $H$ is a sequence of steps. Given a history $H$, we use $H | p$ to denote the subhistory of $H$ containing all and only the events performed by process $p$. Similarly, $H | O$ denotes the subhistory of $H$ containing all and only the events performed on object $O$, plus crash and recovery events.
A response step is \textit{matching} with respect to an invocation step $s$ by a process $p$ on object $X$ in a history $H$ if it is the first response step by $p$ on $X$ that follows $s$ in $H$, and it occurs before $p$'s next invocation (if any) in $H$.

Given a history $H$ and a process $p$, an \textit{operation} by $p$ in $H$ comprises an invocation step and its matching response, if it exists. An operation is \textit{complete} if it has a matching response step, and \textit{pending} otherwise. Given two operations $op_1$ and $op_2$ in a history $H$, we say that $op_1$ \textit{happens before} $op_2$, denoted by $op_1 <_H op_2$, if $op_1$ has a matching
response that precedes the invocation step of $op_2$ in $H$. If neither $op_1 <_H op_2$ nor $op_2 <_H op_1$ holds then we say that $op_1$ and $op_2$ are \textit{concurrent} in $H$.

A history $H$ is \textit{sequential} if no two operations in it are concurrent. Two histories $H$ and $H'$ are \textit{equivalent} if for every process $p$, $H|p = H'|p$ holds. A history $H$ is \textit{well-formed} if for each process $p$, each invocation step in $H|p$ is immediately followed by a matching response, or by a crash step, an every response step in $H|p$ is a matching response for a preceded invocation. Informally, $H$ is well-formed if $H|p$ is a sequential history of operations, except for the ones that may not have a response step due to crash steps.

An object $O$ is defined using a \textit{sequential specification} which defines its allowed behaviors and is expressed as a set of possible sequential histories over $O$. A sequential history $H$ is legal if for every implemented object $O$ accessed in $H$, $H|O$ belongs to the sequential specification of $O$.


\subsubsection{Correcntess Conditions}
We now consider different variants of correctness conditions for NVRAM systems which takes into consideration failures. We follow Berryhill et al. \cite{DBLP:conf/opodis/BerryhillGT15} for formal definitions. For each variant, given a history $H$, we first define a way to extend $H$ such that some pending operations are supplied with a matching response, and the rest pending operations are removed, followed by a definition containing requirements from the resulted extension.

All the following variants are in some sense a natural extension for the Herlihy and Wing's linearizability property, since it is widely used in conventional shared memory models. As such, we first give a formal definition for linearizability. However, linearizability does not support crash steps, and therefore the definition is valid for history $H$ which is free of crash steps. Given such a history $H$, a \textit{completion} of $H$ is a history $H'$ constructed from $H$ by appending matching responses for a subset of pending operations, and then removing any remaining pending operations.

\begin{definition} [Linearizability]
	\label{Definition: Linearizability}
	A finite history $H$ is linearizable if it has no crush events, and it has a completion $H'$ and there exists a legal sequential history $S$ such that:
	\begin{enumerate}
		\item [L1.] $H'$ is equivalent to $S$; and
		\item [L2.] $<_H \subseteq <_S$ (i.e., if $op_1 <_H op_2$ and both ops appear in $S$ then $op_1 <_S op_2$).
	\end{enumerate}
\end{definition}

For strict linearizability, a \textit{strict completion} of $H$ is a history $H'$ constructed from $H$ by inserting matching responses for a subset of pending operations after the operation’s invocation and before the next crash step (if any), and finally removing any remaining pending operations and crash steps.

\begin{definition} [Strict linearizability]
	\label{Definition: Strict linearizability} 
	A finite history $H$ is strictly linearizable if it has a strict completion $H'$ and there exists a legal sequential history $S$ such that:
	\begin{enumerate}
		\item [SL1.] $H'$ is equivalent to $S$; and
		\item [SL2.] $<_{H'} \subseteq <_S$ (i.e., if $op_1 <_{H'} op_2$ and both ops appear in $S$ then $op_1 <_S op_2$).
	\end{enumerate}
\end{definition}

For persistent linearizability, a \textit{persistent completion} of $H$ is a history $H'$ constructed from $H$ by inserting matching responses for a subset of pending operations after the operation’s invocation and before the next invocation step of the same process, and finally removing any remaining pending operations and crash steps.

\begin{definition} [Persistent linearizability]
	\label{Definition: Persistent linearizability}
	A finite history $H$ is persistently linearizable if it has a persistent completion $H'$ and there exists a legal sequential history $S$ such that both conditions SL1 and SL2 of definition \ref{Definition: Strict linearizability} holds.
\end{definition}


For recoverable linearizability, a \textit{recoverable completion} $H'$ is obtained from $H$ in exactly the same manner as a strict completion. In addition, the \textit{invoked before} relation over a history $H$, denoted $\ll_H$ is an extension of the "happens before" relation, such that $op_1 \ll_H op_2$ if $op_1 <_H op_2$ or that both operations invoked by the same process $p$ on the same object $X$, and the invocation step of $op_1$ precedes the invocation step of $op_2$ in $H$.

\begin{definition} [Recoverable linearizability]
	\label{Definition: Recoverable linearizability}
	A finite history $H$ is recoverable linearizable if it has a recoverable completion $H'$ and there exists a legal sequential history $S$ such that:
	\begin{enumerate}
		\item [RL1.] $H'$ is equivalent to $S$; and
		\item [RL2.] $\ll_H \subseteq <_S$ (i.e., if $op_1 \ll_H op_2$ and both ops appear in $S$ then $op_1 <_S op_2$).
	\end{enumerate}
\end{definition}





\paragraph{Base Objects and Primitive Operations}
Concurrent applications are programs that use \emph{base objects}.
In the simplest case, these are the shared memory locations provided by the multi-core system.

The system's hardware or operating system provide \emph{primitive operations} that can be applied to memory locations. The simplest primitives, which are always assumed,
are the familiar \emph{read} and \emph{write} operations,
also called \emph{load} and \emph{store}.
%It has been shown \cite{herlihy91waitfree} that reads and writes alone
%cannot support fault-tolerant implementations of even
%the most basic data structures, like queues and stacks. This and other evidence on the relative weakness of
Modern architectures also provide stronger atomic \emph{read-modify-write} primitives. The most notable of these is \emph{compare-and-swap} (abbreviated \textit{CAS}), which takes three arguments: a memory address, an old value, and a new value. If the address stores the old value, it is replaced with the new value; otherwise it is unchanged. The success or failure of this operation is then reported back to the program.
%It is crucial that this operation is executed atomically; thus an algorithm can read a datum from memory, modify it, and write it back
%only if no other thread modified it in the meantime.
%
%To exemplify the use of the \textit{CAS} operation, consider the following implementation of a concurrent \emph{counter} object. A counter supports a single operation \texttt{INC}, which atomically increments the counter value by $1$ and returns its previous value. Using \textit{CAS}, the \texttt{INC} operation can be implemented by reading the current
%counter value (old value), adding 1, and then using \textit{CAS} for attempting to update the counter. If no other
%thread modifies the counter between the read and \textit{CAS} operations, the \textit{CAS} will succeed, and the counter will be updated. However, if a concurrent modification does occur, the \textit{CAS} will fail, and the thread
%will repeatedly retry the update (by first reading the new value of the counter) until it is successful. This algorithm is nonblocking but not wait-free, since other threads may keep
%incrementing the counter and make our thread repeatedly fail.
%
%\textit{CAS} can be used, together with reads and writes,
%to implement any object in a wait-free manner~\cite{herlihy91waitfree}.
%It has consequently became a synchronization primitive of choice, and hardware support for
%it is provided in many multiprocessor architectures, e.g.,~\cite{Itanium2001,Motorola86,Sparc}.

Another example of a widely-implemented strong primitive is \emph{fetch-and-add}, which takes a memory address and an integer as its two arguments. When applied, it atomically adds the integer argument to the value stored at the memory address and returns the previous value.
%A concurrent counter can be trivially implemented by using a single shared variable that supports the \emph{fetch-and-inc} operation.

%Although atomic read-modify-write primitives, such as CAS and \emph{fetch-and-inc}, are extremely useful for %inter-thread synchronization, they are typically substantially slower than regular reads and writes.
%On many shared-memory multiprocessors,
%they cause the execution of a memory fence
%(see, e.g., \cite[Section 8.1.2.2]{intelsys}).

\paragraph{Correctness Conditions}
When the application implements a concurrent data structure, it is necessary to specify its \emph{semantics}, namely, the properties provided by values it returns, which
determine the feasibility and complexity of implementing it. The \emph{correctness} condition of a concurrent data structure specifies how to derive the semantics of concurrent implementations of the data structure from the corresponding \emph{sequential specification} of the data structure.
This requires to disambiguate the expected results of concurrent operations on
the data structure.

Two common ways to do so are \emph{sequential consistency}~\cite{LamportSC}
and \emph{linearizability}~\cite{herlihyWingLinearizability}.
Both require that the values returned by the operations appear to have been returned by a sequential
execution of the same operations; sequential consistency only requires this order to be consistent with
the order in which each individual thread invokes the operations, while linearizability further requires
this order to be consistent with the real-time order of non-overlapping operations. The standard shared-memory model assumes that shared memory is linearizable or at least sequentially consistent.

\remove{%%%This abstracts away the fact that in real-world systems, main memory is not even sequentially consistent. This abstraction facilitated, however, capturing many of the theoretical limitations of real-world systems, as well as devising concurrent algorithms that can be adapted to work on real-world systems relatively easily (e.g. by adding barrier instructions wherever required).
}%%% END REMOVE

\paragraph{Progress Guarantees}
Progress (also called \emph{liveness}) guarantees specify the conditions under which operations on a concurrent data-structure terminate. With \emph{blocking} (lock-based) implementations, the failure of a single thread may halt the progress of all other threads, if the thread fails while holding a lock. Consequently, the progress of blocking implementations can be guaranteed only when every participating thread is \emph{live}. (A thread is live if, whenever it begins executing an algorithm, it continues to take steps until the algorithm terminates.)
%However, even blocking algorithms are expected to be \emph{weakly progressive}~\cite{DBLP:conf/popl/GuerraouiK09},
%guaranteeing that an operation that does not encounter conflicts terminates successfully.
%[[Took out definition of \emph{weakly progressive}.]]

When locks are not used, threads should be able to make progress despite failures of other threads.
There are several formal definitions capturing this intuitive requirement.
The strongest liveness guarantee, \emph{wait-freedom}~\cite{herlihy91waitfree},
assures that every operation performed by a thread completes within
a finite number of its own steps, regardless of the failures of other threads.
%\emph{Lock-free} algorithms \cite{herlihy91waitfree} may be unfair to individual threads, and only guarantee that \emph{some} operation completes within a finite number of steps, regardless of thread failures. Clearly, all
%wait-free algorithms are lock-free, but, in general, a lock-free algorithm is not wait-free.
%
%\remove{
%\emph{Nonblocking} algorithms \cite{herlihy91waitfree} (also called \emph{lock-free})
%may be unfair to individual threads, and only guarantee that \emph{some} operation completes within a finite number of steps, regardless of thread failures. Clearly, all
%wait-free algorithms are nonblocking, but, in general, a nonblocking algorithm is not wait-free.
%[[[We can skip this paragraph.]]]}
%%
The weaker \emph{obstruction-freedom} guarantee~\cite{AttiyaGHK09,HerlihyLMS03, DBLP:conf/icdcs/HerlihyLM03}
ensures only that if a thread executes in isolation (from some point on),
without interleaved steps of other threads,
then it will complete its operation within a finite number of steps.
%Obstruction-free algorithms are also weakly progressive.
%[[Took out reference to weakly progressive.]]
%Wait-free, lock-free and obstruction-free algorithms fall under the category of \emph{nonblocking} algorithms \cite{herlihy91waitfree}, algorithms that ensure progress while avoiding the use of locks.
%All wait-free algorithms are also obstruction-free, but the converse does not hold, in general.


\subsection{Crash-Recovery Model}
\label{subsec:progressCorrectness}

Linearizability is ill-equipped to specify the correctness criteria for
implementations that support crash-recovery failures,
since linearizability has no notion of an aborted or failed operation,
and requires that a process finish one operation before it invokes the next.
Extended versions of linearizability or, more generally,
alternative definitions are required for
specifying correctness for such implementations.
%[[it is done using an extension of linearizability.]]

Aguilera and Fr{\o}lund~\cite{Aguilera2003StrictLA}
proposed \emph{strict linearizability} as a correctness condition
for persistent concurrent objects,
which treats the crash of a process as a response,
either successful or unsuccessful, to the interrupted operation.
A successful response means that the operation takes effect at some point
between its invocation and the crash failure,
and an unsuccessful response means that the operation does not take effect at all.
Strict linearizability preserves both \emph{locality}~\cite{herlihy91waitfree}
and program order.
However, they proved that it precludes the implementation of
some wait-free objects for certain machine models:
there is no wait-free implementations of multi-reader single-writer (MRSW)
registers from single-reader single-writer (SRSW) registers under strict linearizability.

Guerraoui and Levy~\cite{DBLP:conf/icdcs/GuerraouiL04}
proposed \emph{persistent atomicity}.
It is similar to strict linearizability, but allows an operation
interrupted by a failure to take effect before
the subsequent invocation of the same process, possibly after the failure.
They also proposed \emph{transient atomicity},
which relaxes this criterion even further and allows an interrupted operation
to take effect before the subsequent write response of the same process.
Both conditions ensure that the state of an object will be consistent
in the wake of a crash, but they do not provide locality:
correct histories of separate objects, when merged,
will not necessarily yield a correct composite history.

Berryhill et al.~\cite{DBLP:conf/opodis/BerryhillGT15} proposed an alternative
condition, called \emph{recoverable linearizability},
which achieves locality but may sacrifice program order after a crash.
It is a relaxed version of persistent atomicity,
which requires the operation to be linearized or aborted before
any subsequent linearization by the pending thread on that same object.

Izraelevitz et al.~\cite{DBLP:conf/wdag/IzraelevitzMS16} considered
a real-world failure model, in which processes are assumed to fail together,
as part of a full-system crash.
Under this model, persistent atomicity and recoverable linearizability
are indistinguishable (and thus local).
The term \emph{durable linearizability} was used to refer to this merged
consistency condition under the restricted failure model.

%Berryhill et al. proposed \emph{recoverable linearizability} (or \emph{R-linearizability}) %\cite{DBLP:conf/opodis/BerryhillGT15}, a correctness condition that allows an operation to be linearized after %the failure that interrupted it. R-linearizability satisfies locality and also maintains program order.


\remove{%%%%%%%%%%
	\subsection{Complexity Measures}
	
	Concurrent algorithms are typically evaluated according to traditional measures of space complexity and (typically worst-case) step complexity: the number of memory locations used, and the number of steps taken in an execution of an implemented operation.
	
	%Sometimes, these measures do not suffice to accurately evaluate implementations and additional
	%measures should be used.
	%; for example, the implementation of a counter using \textit{fetch-and-inc}, mentioned above, has
	%For example, when all threads access the same memory location,
	%a large concentration of primitives are applied on the same base object,
	%but the traditional step complexity is exactly $1$.
	%The cost is more accurately captured by the number of \emph{memory stalls}
	%incurred by a thread \cite{DBLP:journals/jacm/DworkHW97},
	%since concurrent (modifying) operations to the same base object are serialized.
	%While this measure is an abstraction of real architectures,
	%it is considered accurate enough to predict the delay
	%incurred by the last primitive to take effect.
	
	%While step complexity is a reasonable measure for algorithms that do not use locks, it is
	Step complexity is problematic as a measure for busy-wait
	blocking synchronization because, in this case,
	a thread may perform an unbounded number of memory accesses
	while busy-waiting for another thread holding the lock.
	Indeed, Alur and Taubenfeld \cite{Alur-Taubenfeld-mutex-results} have shown that even the first thread to enter the critical section can be made to perform an unbounded number of accesses. Instead, the time complexity of blocking algorithms is typically measured by counting only \emph{remote memory references} (RMRs), i.e., memory accesses that traverse the processor-to-memory interconnect.
	
	%Contemporary shared-memory mutual exclusion research focuses on local-spin algorithms, in which all
	%busy-waiting is done by means of read-only loops that repeatedly
	%test locally accessible variables, and uses the RMRs metric \cite{anderson-sharedmemory}.
	
} %%%%%%%%%%% END REMOVE
